{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c759fd-1081-4e7a-809a-1009ff1b7e43",
   "metadata": {},
   "source": [
    "# Applying One-Hot Encoding and Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43736459-b5ac-49b7-b83d-0cd12c58d821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 18:20:56.904887: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-01 18:20:57.270314: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-01 18:20:57.271912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-01 18:20:58.507397: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 0.6879 - accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6793 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6707 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6622 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6538 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6455 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6372 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6290 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6209 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6128 - accuracy: 1.0000\n",
      "dogs: [ 0.03083587  0.01999585 -0.04435885 -0.00131087 -0.02537207 -0.00349475\n",
      "  0.00086652  0.01861478  0.03325668 -0.05791751  0.05910502  0.00358215\n",
      " -0.02887191  0.00640361  0.03506241  0.00209469  0.02788948 -0.01956146\n",
      "  0.02115652  0.01553223 -0.03646922  0.00412313 -0.03897573 -0.00051035\n",
      "  0.01889807 -0.01284106 -0.05835516 -0.0030121   0.0285296  -0.04742509\n",
      " -0.03990437  0.01890193  0.04802112 -0.00081431 -0.0243078  -0.03775312\n",
      " -0.01541958 -0.03932305  0.0086851   0.01086652 -0.03926727  0.00294969\n",
      "  0.01014555 -0.05232569  0.04622016 -0.0081026   0.05375386  0.02431481\n",
      "  0.03808679  0.00553927]\n",
      "i: [-0.05774513 -0.0380602   0.0103313   0.03932075 -0.00953235 -0.04939635\n",
      " -0.03473277 -0.05126707  0.01445158 -0.00678062 -0.03994674  0.02027858\n",
      " -0.0406269  -0.00640239 -0.0070122   0.0204288  -0.05706678  0.05145664\n",
      " -0.0248176  -0.00274139 -0.03227123  0.03088594 -0.02534124  0.01945266\n",
      " -0.03014624 -0.03064462 -0.0304117   0.0357402   0.04186422  0.02215781\n",
      "  0.00734686 -0.02600588 -0.02030201  0.02720269  0.01974154  0.02510124\n",
      " -0.01080772  0.02626422 -0.04401018  0.00315966 -0.05218643  0.01207417\n",
      " -0.02821435  0.01987829 -0.0079719  -0.04646051 -0.00206176  0.00457855\n",
      " -0.01263302  0.03028384]\n",
      "cats: [ 0.0237386   0.05421167 -0.05657886 -0.05833164  0.03559212 -0.02341459\n",
      "  0.01170299 -0.01655731 -0.03855918 -0.05091546 -0.01820147 -0.00268871\n",
      " -0.01362331 -0.00971022 -0.01557424  0.03688768  0.03381287 -0.0045228\n",
      " -0.04574518 -0.04092748  0.011444    0.04755241  0.03629299  0.03417749\n",
      " -0.01691564  0.05712848 -0.01791612 -0.00530061 -0.0252136   0.02626749\n",
      " -0.01716362  0.015086    0.02153619 -0.01837213 -0.03088501  0.033145\n",
      "  0.03621363 -0.03179856  0.01607662  0.01114966  0.00688306 -0.0487872\n",
      " -0.01002535 -0.03139825  0.0061243   0.00210105 -0.03446169  0.02761806\n",
      "  0.03692688 -0.03435686]\n",
      "are: [-0.00729011  0.05063921  0.01227711 -0.00339881  0.02730847  0.03981894\n",
      " -0.00890515  0.02606673  0.01539955 -0.02696471  0.05889728 -0.01735868\n",
      "  0.02564306 -0.00717783  0.03335584 -0.04217037 -0.00531942  0.02300888\n",
      " -0.02017684 -0.02452286  0.0323994  -0.01085733  0.03410025 -0.01229178\n",
      " -0.02421742  0.03749948  0.03415635 -0.0277594   0.01097909  0.00203078\n",
      "  0.04114924 -0.01411157  0.02920776  0.00719633  0.04477092 -0.02606605\n",
      " -0.01642479 -0.03629781  0.00969864 -0.02968429 -0.03918018  0.03399834\n",
      " -0.01520197 -0.0055696  -0.0268947  -0.00328958  0.02634263  0.00338763\n",
      " -0.03965785  0.01953811]\n",
      "love: [ 0.04015592 -0.05716085 -0.04518467 -0.02615501  0.03518486  0.03116778\n",
      "  0.00058747  0.02189748  0.01798048  0.02980602  0.00133568 -0.011037\n",
      " -0.04962076 -0.05285819 -0.00712286 -0.05643211 -0.02971622 -0.05530854\n",
      " -0.01115533 -0.03660469  0.02721779  0.01361734 -0.02695559  0.01383858\n",
      " -0.03401155  0.05172771  0.05288502 -0.01461533  0.05214813  0.01894606\n",
      " -0.04731767 -0.02635938 -0.03503126  0.03022429  0.02658809  0.02499736\n",
      "  0.02653755  0.02091079  0.00211241  0.05002351  0.0314504  -0.03497835\n",
      "  0.02927043 -0.05212256 -0.01796555 -0.00395094  0.00794496 -0.04181967\n",
      "  0.00573778  0.02128231]\n",
      "adore: [-0.01222437 -0.02946324  0.00601999  0.00941505  0.05778271 -0.03048374\n",
      " -0.00895493  0.04531304 -0.00772357 -0.01699962 -0.05515067 -0.0177837\n",
      " -0.05229419 -0.02213513 -0.04998065 -0.01928254  0.0421135   0.00372867\n",
      " -0.0559222   0.00908863  0.03157606 -0.03728216 -0.00688054  0.00168494\n",
      " -0.03439806 -0.0379957  -0.01416577 -0.01020014 -0.03225628  0.04186462\n",
      "  0.03403037 -0.00492018  0.00808655 -0.01062731  0.02290065 -0.00126074\n",
      " -0.04303994 -0.01996839 -0.02254271  0.03123348 -0.00656349  0.00887386\n",
      "  0.01108167 -0.05797962  0.0230521   0.01273276  0.05898567 -0.00689112\n",
      " -0.00572299 -0.02501791]\n",
      "the: [-0.01021158 -0.01419334  0.05268697  0.02004996  0.00706123  0.01702885\n",
      " -0.02303293 -0.05789439 -0.02037175 -0.01607267 -0.01648256 -0.0104697\n",
      " -0.02022689 -0.01712196 -0.01300538 -0.01097032 -0.01096484 -0.00973154\n",
      "  0.02692633 -0.02587011 -0.00465302 -0.03698951 -0.0396362   0.031432\n",
      "  0.02944499 -0.02003322 -0.02948132 -0.03845987  0.02254211  0.02658877\n",
      "  0.04631669 -0.05694642 -0.01924534 -0.01353027  0.02902558  0.02191196\n",
      "  0.04567555  0.0325131   0.00662164  0.00511217 -0.04266594 -0.03817221\n",
      "  0.0541785   0.03124494 -0.05633266  0.00481817 -0.01155909  0.03648093\n",
      " -0.04259363 -0.02865408]\n",
      "best: [-0.00941926 -0.05660921 -0.00555218  0.05686529 -0.04339043  0.0387811\n",
      "  0.01521662 -0.0397029  -0.00160473  0.03899791  0.02576657 -0.03525819\n",
      " -0.02442083  0.04333076 -0.04224522 -0.00675824 -0.03672264 -0.02224547\n",
      "  0.02105031  0.05882082  0.0584716   0.01802189  0.01953673 -0.03219754\n",
      "  0.00099599 -0.01757298  0.05806108  0.02558089  0.00408892  0.05427514\n",
      " -0.01663206 -0.00207523 -0.00977111 -0.03069168  0.04271466  0.03371886\n",
      "  0.02587492 -0.04011738  0.04389364  0.05663276 -0.02158178  0.01683281\n",
      "  0.03632382 -0.00941237  0.01138616  0.0367391  -0.00187235  0.01039556\n",
      "  0.00674505 -0.03999359]\n",
      "and: [ 0.00214038 -0.01637641 -0.00570147 -0.03011274  0.03367507 -0.01033247\n",
      "  0.03611321  0.02788284 -0.03486254 -0.0028035  -0.00227077  0.01880167\n",
      " -0.03318975  0.00480055 -0.00483107 -0.0425152   0.05608803 -0.00320852\n",
      " -0.01101807  0.02475619 -0.04017257 -0.02752915 -0.01886345  0.0168542\n",
      " -0.01873315 -0.03117994 -0.05657613 -0.00095524 -0.05175787  0.00503611\n",
      " -0.02938875  0.01771205  0.05653769 -0.01898959 -0.01111773  0.02359934\n",
      "  0.00175001 -0.0343835  -0.02087682  0.05006729 -0.03494067 -0.05480131\n",
      "  0.03424665 -0.03272016  0.01074483  0.02859135 -0.01051821  0.01983403\n",
      "  0.02502319 -0.00316917]\n",
      "friends: [-0.0366797  -0.05133355  0.04684313  0.04991492  0.01179279  0.01269342\n",
      "  0.01385341 -0.03404946 -0.0035906  -0.02696022  0.0316835  -0.02258866\n",
      "  0.03778142 -0.00033612 -0.04223994 -0.04744947  0.02807457 -0.00582306\n",
      " -0.03326349  0.05808556 -0.03576039  0.0559492   0.00528321 -0.02440226\n",
      "  0.03159088  0.02540131  0.00326748  0.01682688  0.00854584 -0.02287911\n",
      "  0.02654791 -0.01453083  0.01931912 -0.03350692 -0.03406629  0.01049944\n",
      " -0.00984433  0.03560914  0.00802324  0.03698097  0.0428933  -0.04488775\n",
      "  0.00130033  0.02678423 -0.01860493 -0.03416742  0.00803962 -0.01107013\n",
      "  0.01094077  0.03349068]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "#input data\n",
    "texts = [\n",
    "    'I love cats',\n",
    "    'I adore dogs',\n",
    "    'Dogs are the best',\n",
    "    'Cats and dogs are friends'\n",
    "]\n",
    "\n",
    "# Tokenize the input texts \n",
    "# convert them into sequences of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Vocabulary size is determined by the number of unique words in the input data\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Padding the sequences to make them of equal length\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "#  embedding dimension\n",
    "embedding_dim = 50\n",
    "\n",
    "# Model building\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_sequence_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Convert target labels to NumPy array\n",
    "labels = np.array([1, 1, 0, 0])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, epochs=10)\n",
    "\n",
    "\n",
    "embedding_matrix = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Print the word embeddings\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    print(f\"{word}: {embedding_matrix[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39770053-0f9e-458d-a1ff-0882f33f2281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
